{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Downloading huggingface_hub-0.23.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.4.28-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
      "Downloading transformers-4.40.2-py3-none-any.whl (9.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.4.28-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (774 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.1/774.1 kB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.23.0 regex-2024.4.28 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.40.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.26.2)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.4.2 scipy-1.13.0 threadpoolctl-3.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.2.2 tzdata-2024.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2TWT19ZbClay"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EdunYKh3IG2Y"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XjuGBv7HEg5u"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and remaining data\n",
    "train_df, remaining_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split remaining data into validation and test sets\n",
    "val_df, test_df = train_test_split(remaining_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CCB9jwdgEvFm",
    "outputId": "ff624dd5-c6a4-42eb-b108-b4efa46053a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'label'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5YkwP-N_IeKS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_253/985258913.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['text'].fillna(\"Missing text\", inplace=True)  # Replace nulls with a placeholder string\n"
     ]
    }
   ],
   "source": [
    "df['text'].fillna(\"Missing text\", inplace=True)  # Replace nulls with a placeholder string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VswdFnJLInyL",
    "outputId": "3fc1d695-55d9-413b-ce38-fdf9a5d64bff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\n",
      "<class 'str'>    38448\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check data types in the text column\n",
    "print(df['text'].apply(type).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sy8HDTQ6Jbno",
    "outputId": "22c84b54-7b40-4f2f-a608-deccd448a21a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels before cleaning: ['Left Wing' 'Neutral' 'Right Wing' nan ' whenever I leave the West'\n",
      " '1/25/2022 18:45'\n",
      " ' and that I may be better off then they are because I still have elders that I can go to who will make me feel at home for a while as they cleanse me. Sometimes I find myself wondering']\n",
      "Unique labels after cleaning: ['Left Wing' 'Neutral' 'Right Wing' nan]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Display unique values before cleaning\n",
    "print(\"Unique labels before cleaning:\", train_df['label'].unique())\n",
    "\n",
    "# Clean labels: Only keep valid categories, set others to NaN\n",
    "valid_labels = ['Left Wing', 'Right Wing', 'Neutral']\n",
    "train_df['label'] = train_df['label'].apply(lambda x: x if x in valid_labels else np.nan)\n",
    "val_df['label'] = val_df['label'].apply(lambda x: x if x in valid_labels else np.nan)\n",
    "test_df['label'] = test_df['label'].apply(lambda x: x if x in valid_labels else np.nan)\n",
    "\n",
    "# Option to drop NaNs if your dataset allows\n",
    "# train_df.dropna(subset=['label'], inplace=True)\n",
    "# val_df.dropna(subset=['label'], inplace=True)\n",
    "# test_df.dropna(subset=['label'], inplace=True)\n",
    "\n",
    "# Display unique values after cleaning\n",
    "print(\"Unique labels after cleaning:\", train_df['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MKBSHsDEJfQI",
    "outputId": "ec346f36-2a36-494d-b3ec-0325f04b296f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded labels: [0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the training data and transform all datasets\n",
    "train_df['label'] = encoder.fit_transform(train_df['label'].astype(str))\n",
    "val_df['label'] = encoder.transform(val_df['label'].astype(str))\n",
    "test_df['label'] = encoder.transform(test_df['label'].astype(str))\n",
    "\n",
    "# Check transformed labels\n",
    "print(\"Encoded labels:\", train_df['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L_GW2bp3JAeN",
    "outputId": "3dc31d58-1a20-4e66-949f-1170ff7e2da1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n",
      "label\n",
      "<class 'int'>    30758\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df['label'].unique())  # Display unique label values\n",
    "print(train_df['label'].apply(type).value_counts())  # Check data types of lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "K55fL2IDKOjc"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "opDMYB8-J4Tb",
    "outputId": "14c646c7-0c16-4f0f-c584-0710cf59ef61"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization successful\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', add_prefix_space=True)\n",
    "\n",
    "def tokenize_data(df):\n",
    "    # Ensure text and labels are in the correct format\n",
    "    texts = df['text'].astype(str).tolist()\n",
    "    labels = df['label'].tolist()\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokenized = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "\n",
    "    # Add labels\n",
    "    tokenized['labels'] = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return tokenized\n",
    "try:\n",
    "    train_dataset = tokenize_data(train_df)\n",
    "    val_dataset = tokenize_data(val_df)\n",
    "    test_dataset = tokenize_data(test_df)\n",
    "    print(\"Tokenization successful\")\n",
    "except Exception as e:\n",
    "    print(f\"Tokenization failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "H49NbCKYJHvG"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx] for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextDataset(train_dataset)\n",
    "val_dataset = TextDataset(val_dataset)\n",
    "test_dataset = TextDataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1OCPyq4LKmDX"
   },
   "outputs": [],
   "source": [
    "model_id = 'bert-base-uncased'\n",
    "# model = BertForSequenceClassification.from_pretrained(model_id, num_labels=len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ecitJUBuF41c",
    "outputId": "8fb40383-912a-418b-f775-c37b320d8e46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of labels: 4\n",
      "the labels: ['LeftWing', 'Neutral', 'RightWing', 'nan']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "# Manually define class names if they are known\n",
    "class_names = ['LeftWing', 'Neutral', 'RightWing', 'nan']  # replace with your actual class names\n",
    "num_labels=len(class_names)\n",
    "# Create id2label mapping\n",
    "id2label = {i: name for i, name in enumerate(class_names)}\n",
    "config = AutoConfig.from_pretrained(model_id, num_labels=len(class_names), id2label=id2label)\n",
    "print(f\"number of labels: {num_labels}\")\n",
    "print(f\"the labels: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "PiQNvIDnKlDm"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizerFast, Trainer, TrainingArguments\n",
    "# from transformers import HfFolder  # if using hub related features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "guU1XJCCNAOU"
   },
   "outputs": [],
   "source": [
    "model_id = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fm2zeXYhNUiF",
    "outputId": "db51f7b3-c45d-46d9-be6e-c2899bfc0c6c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the model with configuration for sequence classification\n",
    "from transformers import BertConfig\n",
    "config = BertConfig.from_pretrained(model_id, num_labels=4)  # Adjust num_labels as per your task\n",
    "model = BertForSequenceClassification.from_pretrained(model_id, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J9tp8xMfN0o4",
    "outputId": "33063dea-7a4c-4a7c-f36f-3af4c06dca26"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.12.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.65.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "pehwenbvNZZl"
   },
   "outputs": [],
   "source": [
    "repository_id = 'harshal-11/Bert-political-classification'  # Change as per your setup\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=repository_id,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=f\"{repository_id}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2\n",
    "    # Uncomment and modify the following lines if using Hugging Face Hub features\n",
    "    # report_to=\"tensorboard\",\n",
    "    # push_to_hub=True,\n",
    "    # hub_strategy=\"every_save\",\n",
    "    # hub_model_id=repository_id,\n",
    "    # hub_token=HfFolder.get_token(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "J8v3TUu1NwMf"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "ohLmcbQkPG6v",
    "outputId": "a6ab9de6-5183-4cf3-87e6-696fbe312506"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19225' max='19225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19225/19225 17:37, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.611200</td>\n",
       "      <td>0.423385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.504000</td>\n",
       "      <td>0.435950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.264400</td>\n",
       "      <td>0.547700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.068900</td>\n",
       "      <td>0.730844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.121100</td>\n",
       "      <td>0.830124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=19225, training_loss=0.2770425759653484, metrics={'train_runtime': 1057.9976, 'train_samples_per_second': 145.359, 'train_steps_per_second': 18.171, 'total_flos': 2.023228791048192e+16, 'train_loss': 0.2770425759653484, 'epoch': 5.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "HK7U47JVPNPB"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='241' max='241' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [241/241 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4233852028846741,\n",
       " 'eval_runtime': 6.932,\n",
       " 'eval_samples_per_second': 554.674,\n",
       " 'eval_steps_per_second': 34.766,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='241' max='241' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [241/241 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4233852028846741,\n",
       " 'eval_accuracy': 0.8392717815344604,\n",
       " 'eval_f1': 0.6212949463433945,\n",
       " 'eval_precision': 0.6361399718641509,\n",
       " 'eval_recall': 0.6115823990104325,\n",
       " 'eval_runtime': 6.9231,\n",
       " 'eval_samples_per_second': 555.386,\n",
       " 'eval_steps_per_second': 34.811}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Replace 'your_token' with the actual token you copied from Hugging Face.\n",
    "os.environ['HF_TOKEN'] = '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'\n",
    "\n",
    "# Use this environment variable when you create the `Trainer` or call `push_to_hub`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "training_args.bin:   0%|          | 0.00/4.98k [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   1%|          | 4.06M/438M [00:00<00:10, 40.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "training_args.bin: 100%|██████████| 4.98k/4.98k [00:00<00:00, 20.5kB/s][A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:   4%|▎         | 15.4M/438M [00:00<00:08, 49.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   5%|▍         | 20.4M/438M [00:00<00:18, 22.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   6%|▋         | 27.4M/438M [00:00<00:12, 31.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   7%|▋         | 32.1M/438M [00:01<00:23, 17.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   9%|▉         | 38.4M/438M [00:01<00:16, 23.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  10%|█         | 44.5M/438M [00:01<00:13, 29.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  11%|█▏        | 49.4M/438M [00:01<00:13, 28.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  13%|█▎        | 56.6M/438M [00:01<00:10, 36.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  14%|█▍        | 63.5M/438M [00:01<00:08, 43.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  16%|█▌        | 69.1M/438M [00:02<00:10, 34.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  17%|█▋        | 75.5M/438M [00:02<00:08, 40.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  18%|█▊        | 80.7M/438M [00:02<00:16, 21.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  20%|█▉        | 87.0M/438M [00:02<00:12, 27.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  22%|██▏       | 94.6M/438M [00:03<00:10, 31.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  23%|██▎       | 99.1M/438M [00:03<00:17, 19.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  24%|██▍       | 106M/438M [00:03<00:13, 25.5MB/s] \u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  26%|██▌       | 112M/438M [00:04<00:14, 22.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  27%|██▋       | 118M/438M [00:04<00:11, 27.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  29%|██▊       | 125M/438M [00:04<00:09, 34.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  30%|██▉       | 130M/438M [00:04<00:10, 30.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  31%|███▏      | 137M/438M [00:04<00:08, 36.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  33%|███▎      | 143M/438M [00:04<00:07, 40.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  34%|███▍      | 148M/438M [00:05<00:08, 34.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  35%|███▌      | 154M/438M [00:05<00:07, 39.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  36%|███▋      | 159M/438M [00:05<00:07, 39.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  37%|███▋      | 164M/438M [00:05<00:12, 22.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  39%|███▉      | 170M/438M [00:05<00:09, 29.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  40%|███▉      | 175M/438M [00:05<00:08, 32.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  41%|████      | 179M/438M [00:06<00:08, 30.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  42%|████▏     | 184M/438M [00:06<00:07, 34.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  43%|████▎     | 190M/438M [00:06<00:06, 39.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  44%|████▍     | 195M/438M [00:06<00:07, 30.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  46%|████▌     | 202M/438M [00:06<00:05, 39.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  47%|████▋     | 208M/438M [00:06<00:05, 43.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  49%|████▊     | 213M/438M [00:06<00:07, 30.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  50%|████▉     | 217M/438M [00:07<00:06, 31.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  51%|█████     | 223M/438M [00:07<00:05, 36.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  52%|█████▏    | 227M/438M [00:07<00:06, 30.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  53%|█████▎    | 234M/438M [00:07<00:05, 37.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  55%|█████▍    | 240M/438M [00:07<00:05, 33.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  56%|█████▌    | 246M/438M [00:07<00:04, 39.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  57%|█████▋    | 252M/438M [00:07<00:04, 42.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  59%|█████▊    | 257M/438M [00:08<00:05, 31.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  60%|██████    | 264M/438M [00:08<00:04, 39.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  62%|██████▏   | 271M/438M [00:08<00:03, 46.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  63%|██████▎   | 277M/438M [00:08<00:04, 33.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  65%|██████▍   | 284M/438M [00:08<00:03, 40.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  66%|██████▌   | 290M/438M [00:09<00:04, 34.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  68%|██████▊   | 297M/438M [00:09<00:03, 42.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  69%|██████▉   | 304M/438M [00:09<00:03, 36.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  71%|███████   | 311M/438M [00:09<00:02, 43.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  73%|███████▎  | 318M/438M [00:09<00:02, 49.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  74%|███████▍  | 324M/438M [00:09<00:02, 40.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  76%|███████▌  | 332M/438M [00:09<00:02, 47.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  77%|███████▋  | 338M/438M [00:10<00:02, 36.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  79%|███████▉  | 345M/438M [00:10<00:02, 43.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  80%|████████  | 352M/438M [00:10<00:02, 36.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  82%|████████▏ | 360M/438M [00:10<00:01, 44.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  84%|████████▍ | 367M/438M [00:10<00:01, 50.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  85%|████████▌ | 374M/438M [00:10<00:01, 42.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  87%|████████▋ | 382M/438M [00:11<00:01, 50.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  89%|████████▊ | 388M/438M [00:11<00:01, 39.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  90%|█████████ | 395M/438M [00:11<00:00, 46.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  92%|█████████▏| 401M/438M [00:11<00:00, 38.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  93%|█████████▎| 409M/438M [00:11<00:00, 46.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  95%|█████████▍| 416M/438M [00:11<00:00, 37.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  97%|█████████▋| 424M/438M [00:12<00:00, 44.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  99%|█████████▊| 432M/438M [00:12<00:00, 52.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors: 100%|██████████| 438M/438M [00:12<00:00, 34.6MB/s]\u001b[A\u001b[A\n",
      "Upload 2 LFS files: 100%|██████████| 2/2 [00:12<00:00,  6.46s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/harshal-11/Bert-political-classification/commit/d80444b85cf577761f0df502d67bebd3b631cc3c', commit_message='Training completed', commit_description='', oid='d80444b85cf577761f0df502d67bebd3b631cc3c', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(training_args.output_dir)\n",
    "trainer.create_model_card()\n",
    "\n",
    "# Push the tokenizer, model, and model card to the hub\n",
    "trainer.push_to_hub(commit_message=\"Training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
