{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "uqh_s1fMUDmb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHSzEVHMiOAF"
   },
   "source": [
    "# Unusual or Irrelevant Columns: **(can be part of noisy data)**\n",
    " There are columns named Unnamed: 7, Unnamed: 8, and Unnamed: 9 with very few non-null values and likely irrelevant data. These columns might be errors in data entry or\n",
    " #**extraction**\n",
    " and typically do not contain useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hV2snEIGiDOg",
    "outputId": "1da856ef-2e94-4399-c123-1edcb657b5f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24446 entries, 0 to 24445\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   title         24444 non-null  object\n",
      " 1   score         24443 non-null  object\n",
      " 2   subreddit     24443 non-null  object\n",
      " 3   url           24443 non-null  object\n",
      " 4   created_utc   24443 non-null  object\n",
      " 5   num_comments  24443 non-null  object\n",
      " 6   label         24443 non-null  object\n",
      " 7   Unnamed: 7    3 non-null      object\n",
      " 8   Unnamed: 8    3 non-null      object\n",
      " 9   Unnamed: 9    1 non-null      object\n",
      "dtypes: object(10)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('Final_merged.csv')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Lm0JwGiidUe"
   },
   "source": [
    "# **Nulll Values (can be part of missing values)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-wC5ErblUlpn",
    "outputId": "4641e9a9-b697-4133-8815-79ad128adede"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in each column:\n",
      "title           2\n",
      "score           3\n",
      "subreddit       3\n",
      "url             3\n",
      "created_utc     3\n",
      "num_comments    3\n",
      "label           3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('Final_merged.csv')\n",
    "\n",
    "data.drop(columns=['Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9'], inplace=True)\n",
    "\n",
    "null_values = data.isnull().sum()\n",
    "print(\"Null values in each column:\")\n",
    "print(null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JnHl-HjvYnT4",
    "outputId": "0db5d52d-1ad1-4b3e-d1f1-858145a5c738"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in each column:\n",
      "title           0\n",
      "score           0\n",
      "subreddit       0\n",
      "url             0\n",
      "created_utc     0\n",
      "num_comments    0\n",
      "label           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data.dropna(inplace=True)\n",
    "null_values = data.isnull().sum()\n",
    "print(\"Null values in each column:\")\n",
    "print(null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9o9Xx2XmeXF6",
    "outputId": "ee5b9012-a998-47f5-eec5-f978e6fb4868"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of duplicate rows (based only on 'title' and 'subreddit'): 3235\n",
      "\n",
      "Sample of duplicate rows (based only on 'title' and 'subreddit'):\n",
      "                                                 title score   subreddit  \\\n",
      "434  Tyrone Scott announces Green Party of England ...    10  GreenParty   \n",
      "443  Tyrone Scott announces Green Party of England ...     5  GreenParty   \n",
      "654  Green Party Presidential Candidate Cornel West...    27  GreenParty   \n",
      "676  Green Party Presidential Candidate Cornel West...    12  GreenParty   \n",
      "915  Pretty precise description of Israel's style o...    42  GreenParty   \n",
      "\n",
      "                                                   url    created_utc  \\\n",
      "434  https://bright-green.org/2022/06/23/exclusive-...  6/27/22 13:38   \n",
      "443  https://bright-green.org/2022/06/23/exclusive-...  6/23/22 16:52   \n",
      "654  https://youtube.com/watch?v=WHLFTQyX9NA&featur...   7/23/23 1:05   \n",
      "676  https://youtube.com/watch?v=daN0sSIsZGI&featur...  7/14/23 17:18   \n",
      "915               https://i.redd.it/e3kyytk763sc1.jpeg   4/3/24 14:11   \n",
      "\n",
      "    num_comments      label  \n",
      "434            0  Left Wing  \n",
      "443            0  Left Wing  \n",
      "654            3  Left Wing  \n",
      "676            2  Left Wing  \n",
      "915            0  Left Wing  \n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates based on 'title' and 'subreddit' columns\n",
    "duplicate_rows = data[data.duplicated(subset=['title', 'subreddit'],keep=False)]\n",
    "\n",
    "# Get the number of duplicate rows\n",
    "num_duplicate_rows = duplicate_rows.shape[0]\n",
    "print(\"Total number of duplicate rows (based only on 'title' and 'subreddit'):\", num_duplicate_rows)\n",
    "\n",
    "# Take a sample of duplicate rows for inspection\n",
    "duplicate_rows_sample = duplicate_rows.head()  # Take a sample of up to 5 rows\n",
    "print(\"\\nSample of duplicate rows (based only on 'title' and 'subreddit'):\")\n",
    "print(duplicate_rows_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3WCkUY9ii_L"
   },
   "source": [
    "# **Duplicate values (can be part of noisy data)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "zgJxOK2qfEmv"
   },
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(subset=['title', 'subreddit'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9TJNZHZyfnYy",
    "outputId": "725ea4a6-a170-41a9-f94c-9d4d9da1c5ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of duplicate rows (based only on 'title' and 'subreddit'): 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates based on 'title' and 'subreddit' columns\n",
    "duplicate_rows = data[data.duplicated(subset=['title', 'subreddit'])]\n",
    "\n",
    "# Get the number of duplicate rows\n",
    "num_duplicate_rows = duplicate_rows.shape[0]\n",
    "print(\"Total number of duplicate rows (based only on 'title' and 'subreddit'):\", num_duplicate_rows)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
