{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5572ce2b-2046-493f-8672-cfdef55c728a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a84b6f5-eb24-41c6-bce7-00179be5c5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "496c99de-d49e-4a6c-9a37-626138c1114b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e641872-8b9d-4015-af19-83cfef4c1921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\n",
      "<class 'str'>    38448\n",
      "Name: count, dtype: int64\n",
      "Unique labels before cleaning: ['Right Wing' 'Left Wing' nan ' whenever I leave the West'\n",
      " ' and that I may be better off then they are because I still have elders that I can go to who will make me feel at home for a while as they cleanse me. Sometimes I find myself wondering'\n",
      " '01/25/2022 18:45:00' 'Neutral']\n",
      "Unique labels after cleaning: ['Right Wing' 'Left Wing' 'Neutral']\n",
      "Encoded labels: [0 1 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3596/3292259076.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['text'].fillna(\"Missing text\", inplace=True)  # Replace nulls with a placeholder string\n"
     ]
    }
   ],
   "source": [
    "# data load\n",
    "df = pd.read_csv(\"cleaned_data.csv\", encoding='ISO-8859-1')\n",
    "df['text'].fillna(\"Missing text\", inplace=True)  # Replace nulls with a placeholder string\n",
    "# Check data types in the text column\n",
    "print(df['text'].apply(type).value_counts())\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Display unique values before cleaning\n",
    "print(\"Unique labels before cleaning:\", df['label'].unique())\n",
    "\n",
    "# Clean labels: Only keep valid categories, set others to NaN\n",
    "valid_labels = ['Left Wing', 'Right Wing', 'Neutral']\n",
    "df['label'] = df['label'].apply(lambda x: x if x in valid_labels else np.nan)\n",
    "\n",
    "# Option to drop NaNs if your dataset allows\n",
    "# train_df.dropna(subset=['label'], inplace=True)\n",
    "# val_df.dropna(subset=['label'], inplace=True)\n",
    "# test_df.dropna(subset=['label'], inplace=True)\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df['label'] = df['label'].fillna('Neutral')\n",
    "\n",
    "# Display unique values after cleaning\n",
    "print(\"Unique labels after cleaning:\", df['label'].unique())\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and remaining data\n",
    "train_df, remaining_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split remaining data into validation and test sets\n",
    "val_df, test_df = train_test_split(remaining_df, test_size=0.5, random_state=42)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the training data and transform all datasets\n",
    "train_df['label'] = encoder.fit_transform(train_df['label'].astype(str))\n",
    "val_df['label'] = encoder.transform(val_df['label'].astype(str))\n",
    "test_df['label'] = encoder.transform(test_df['label'].astype(str))\n",
    "\n",
    "# Check transformed labels\n",
    "print(\"Encoded labels:\", train_df['label'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef16c117-f69b-47b0-97b8-54eec92cc660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TextDataset(Dataset):\n",
    "#     def __init__(self, encodings,labels):\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.texts = dataframe['text'].tolist()\n",
    "#         self.labels = dataframe['label'].tolist()\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "        \n",
    "#         item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "#         item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "#         return item\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92df5806-9365-4a5d-8bcd-217d1b1ee86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# def tokenize_data(df, tokenizer):\n",
    "#     texts = df['text'].astype(str).tolist()  # Convert text data to string\n",
    "#     labels = df['label'].tolist()  # Extract labels\n",
    "\n",
    "#     # Tokenize the text\n",
    "#     tokenized = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "#     tokenized['labels'] = torch.tensor(labels, dtype=torch.long)  # Add labels to the tokenized data\n",
    "\n",
    "#     return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18320abc-e190-48f9-be58-b2fe35970ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import DistilBertTokenizer\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        self.data = self.tokenize_data(df)\n",
    "    \n",
    "    def tokenize_data(self, df):\n",
    "        texts = df['text'].astype(str).tolist()  # Ensure text data is in string format\n",
    "        labels = df['label'].tolist()  # Extract labels\n",
    "        \n",
    "        # Tokenize the text data\n",
    "        tokenized = self.tokenizer(texts, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "        tokenized['labels'] = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "        return tokenized\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.data.items()}\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca9c62ba-ef40-4b23-b0e2-b4aff273abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# train_dataset = tokenize_data(train_df)  # Assuming train_df is your DataFrame with training data\n",
    "train_dataset = TextDataset(train_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87c9b5ef-95cc-4a6a-96db-114d67bec678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "val_dataset = TextDataset(val_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f720536-a169-4149-bd8c-d5d532a8d322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3977c249-c1ca-43b5-9a0d-8c67bf594d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_id = \"harshal-11/DistillBERT-Political-Finetune\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=repository_id,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs = 5,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=f\"{repository_id}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9289838-1174-4d1a-af31-d4282ba86dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'distilbert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d5434f07-3468-4439-8679-0855cdf27e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of labels: 3\n",
      "the labels: ['LeftWing', 'Neutral', 'RightWing']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "# Manually define class names if they are known\n",
    "class_names = ['LeftWing', 'Neutral', 'RightWing']  # replace with your actual class names\n",
    "num_labels=len(class_names)\n",
    "# Create id2label mapping\n",
    "id2label = {i: name for i, name in enumerate(class_names)}\n",
    "config = AutoConfig.from_pretrained(model_id, num_labels=len(class_names), id2label=id2label)\n",
    "print(f\"number of labels: {num_labels}\")\n",
    "print(f\"the labels: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7f3f848-845a-48f1-94c8-8ed70ad13793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', config = config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f00aebea-d1d1-4cea-9ca5-9bdd53f891b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "472adbf5-0ebe-47d9-80d0-39ea01b41086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19225' max='19225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19225/19225 09:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.554700</td>\n",
       "      <td>0.391648</td>\n",
       "      <td>0.837971</td>\n",
       "      <td>0.828685</td>\n",
       "      <td>0.840992</td>\n",
       "      <td>0.819487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.506100</td>\n",
       "      <td>0.495741</td>\n",
       "      <td>0.852276</td>\n",
       "      <td>0.840907</td>\n",
       "      <td>0.855040</td>\n",
       "      <td>0.830913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.170900</td>\n",
       "      <td>0.573231</td>\n",
       "      <td>0.857477</td>\n",
       "      <td>0.844851</td>\n",
       "      <td>0.854147</td>\n",
       "      <td>0.837563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.096500</td>\n",
       "      <td>0.693301</td>\n",
       "      <td>0.855917</td>\n",
       "      <td>0.843764</td>\n",
       "      <td>0.847762</td>\n",
       "      <td>0.840618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.063000</td>\n",
       "      <td>0.846217</td>\n",
       "      <td>0.858257</td>\n",
       "      <td>0.844365</td>\n",
       "      <td>0.845563</td>\n",
       "      <td>0.843268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=19225, training_loss=0.25989047006426574, metrics={'train_runtime': 572.0776, 'train_samples_per_second': 268.827, 'train_steps_per_second': 33.606, 'total_flos': 1.018626227394048e+16, 'train_loss': 0.25989047006426574, 'epoch': 5.0})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b5c18981-ea12-4d47-b2f2-0fd1e4d2b153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='241' max='241' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [241/241 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.39164817333221436,\n",
       " 'eval_accuracy': 0.8379713914174253,\n",
       " 'eval_f1': 0.8286850094263224,\n",
       " 'eval_precision': 0.8409915832009255,\n",
       " 'eval_recall': 0.8194867492438821,\n",
       " 'eval_runtime': 3.7256,\n",
       " 'eval_samples_per_second': 1032.055,\n",
       " 'eval_steps_per_second': 64.688,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "74d9999f-93c3-4187-8f3d-2f74d628816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Replace 'your_token' with the actual token you copied from Hugging Face.\n",
    "os.environ['HF_TOKEN'] = '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'\n",
    "\n",
    "# Use this environment variable when you create the `Trainer` or call `push_to_hub`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b3c9ec5a-2c9f-4a54-bb46-00b428564d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "training_args.bin:   0%|          | 0.00/4.98k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "training_args.bin: 100%|██████████| 4.98k/4.98k [00:00<00:00, 33.5kB/s][A\n",
      "\n",
      "model.safetensors:   4%|▎         | 9.43M/268M [00:00<00:05, 49.7MB/s]\u001b[A\n",
      "model.safetensors:   5%|▌         | 14.4M/268M [00:00<00:05, 44.4MB/s]\u001b[A\n",
      "model.safetensors:   7%|▋         | 18.8M/268M [00:00<00:12, 20.7MB/s]\u001b[A\n",
      "model.safetensors:  10%|▉         | 25.9M/268M [00:00<00:07, 30.5MB/s]\u001b[A\n",
      "model.safetensors:  12%|█▏        | 32.0M/268M [00:01<00:08, 28.2MB/s]\u001b[A\n",
      "model.safetensors:  14%|█▍        | 37.4M/268M [00:01<00:06, 33.1MB/s]\u001b[A\n",
      "model.safetensors:  16%|█▌        | 42.4M/268M [00:01<00:06, 36.6MB/s]\u001b[A\n",
      "model.safetensors:  18%|█▊        | 47.1M/268M [00:01<00:05, 39.3MB/s]\u001b[A\n",
      "model.safetensors:  19%|█▉        | 51.8M/268M [00:01<00:07, 29.4MB/s]\u001b[A\n",
      "model.safetensors:  22%|██▏       | 57.6M/268M [00:01<00:05, 35.3MB/s]\u001b[A\n",
      "model.safetensors:  23%|██▎       | 62.1M/268M [00:01<00:05, 36.2MB/s]\u001b[A\n",
      "model.safetensors:  25%|██▍       | 66.3M/268M [00:02<00:10, 20.0MB/s]\u001b[A\n",
      "model.safetensors:  27%|██▋       | 71.3M/268M [00:02<00:08, 24.6MB/s]\u001b[A\n",
      "model.safetensors:  29%|██▉       | 77.8M/268M [00:02<00:06, 29.0MB/s]\u001b[A\n",
      "model.safetensors:  31%|███       | 81.7M/268M [00:03<00:10, 17.5MB/s]\u001b[A\n",
      "model.safetensors:  32%|███▏      | 86.9M/268M [00:03<00:08, 22.0MB/s]\u001b[A\n",
      "model.safetensors:  35%|███▍      | 92.6M/268M [00:03<00:06, 27.5MB/s]\u001b[A\n",
      "model.safetensors:  36%|███▌      | 96.8M/268M [00:03<00:07, 23.0MB/s]\u001b[A\n",
      "model.safetensors:  38%|███▊      | 102M/268M [00:03<00:05, 28.5MB/s] \u001b[A\n",
      "model.safetensors:  40%|███▉      | 107M/268M [00:03<00:05, 31.6MB/s]\u001b[A\n",
      "model.safetensors:  42%|████▏     | 112M/268M [00:04<00:05, 27.4MB/s]\u001b[A\n",
      "model.safetensors:  44%|████▍     | 117M/268M [00:04<00:04, 32.3MB/s]\u001b[A\n",
      "model.safetensors:  46%|████▌     | 123M/268M [00:04<00:03, 37.4MB/s]\u001b[A\n",
      "model.safetensors:  48%|████▊     | 128M/268M [00:04<00:04, 30.5MB/s]\u001b[A\n",
      "model.safetensors:  50%|████▉     | 134M/268M [00:04<00:03, 35.7MB/s]\u001b[A\n",
      "model.safetensors:  52%|█████▏    | 138M/268M [00:04<00:03, 38.4MB/s]\u001b[A\n",
      "model.safetensors:  54%|█████▎    | 144M/268M [00:04<00:02, 41.9MB/s]\u001b[A\n",
      "model.safetensors:  55%|█████▌    | 148M/268M [00:04<00:03, 33.0MB/s]\u001b[A\n",
      "model.safetensors:  57%|█████▋    | 153M/268M [00:05<00:03, 36.2MB/s]\u001b[A\n",
      "model.safetensors:  59%|█████▉    | 158M/268M [00:05<00:02, 39.8MB/s]\u001b[A\n",
      "model.safetensors:  61%|██████    | 163M/268M [00:05<00:03, 31.9MB/s]\u001b[A\n",
      "model.safetensors:  63%|██████▎   | 168M/268M [00:05<00:02, 36.4MB/s]\u001b[A\n",
      "model.safetensors:  65%|██████▍   | 174M/268M [00:05<00:02, 40.9MB/s]\u001b[A\n",
      "model.safetensors:  67%|██████▋   | 178M/268M [00:05<00:03, 28.2MB/s]\u001b[A\n",
      "model.safetensors:  69%|██████▊   | 184M/268M [00:05<00:02, 33.5MB/s]\u001b[A\n",
      "model.safetensors:  70%|███████   | 189M/268M [00:06<00:02, 36.9MB/s]\u001b[A\n",
      "model.safetensors:  72%|███████▏  | 193M/268M [00:06<00:02, 28.5MB/s]\u001b[A\n",
      "model.safetensors:  74%|███████▍  | 199M/268M [00:06<00:02, 33.6MB/s]\u001b[A\n",
      "model.safetensors:  76%|███████▌  | 204M/268M [00:06<00:01, 38.3MB/s]\u001b[A\n",
      "model.safetensors:  78%|███████▊  | 209M/268M [00:06<00:02, 23.4MB/s]\u001b[A\n",
      "model.safetensors:  80%|███████▉  | 214M/268M [00:07<00:01, 28.0MB/s]\u001b[A\n",
      "model.safetensors:  82%|████████▏ | 219M/268M [00:07<00:01, 32.1MB/s]\u001b[A\n",
      "model.safetensors:  84%|████████▎ | 224M/268M [00:07<00:01, 30.6MB/s]\u001b[A\n",
      "model.safetensors:  86%|████████▌ | 229M/268M [00:07<00:01, 35.5MB/s]\u001b[A\n",
      "model.safetensors:  88%|████████▊ | 235M/268M [00:07<00:00, 39.3MB/s]\u001b[A\n",
      "model.safetensors:  90%|████████▉ | 240M/268M [00:07<00:00, 33.2MB/s]\u001b[A\n",
      "model.safetensors:  92%|█████████▏| 246M/268M [00:07<00:00, 37.9MB/s]\u001b[A\n",
      "model.safetensors:  94%|█████████▍| 251M/268M [00:07<00:00, 42.7MB/s]\u001b[A\n",
      "model.safetensors:  96%|█████████▌| 256M/268M [00:08<00:00, 23.1MB/s]\u001b[A\n",
      "model.safetensors:  98%|█████████▊| 262M/268M [00:08<00:00, 28.5MB/s]\u001b[A\n",
      "model.safetensors: 100%|██████████| 268M/268M [00:08<00:00, 30.4MB/s]\u001b[A\n",
      "Upload 2 LFS files: 100%|██████████| 2/2 [00:09<00:00,  4.53s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/harshal-11/DistillBERT-Political-Finetune/commit/4e218bdcc3c69844bcb8aab1bf218e7942292222', commit_message='Training completed', commit_description='', oid='4e218bdcc3c69844bcb8aab1bf218e7942292222', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(training_args.output_dir)\n",
    "trainer.create_model_card()\n",
    "\n",
    "# Push the tokenizer, model, and model card to the hub\n",
    "trainer.push_to_hub(commit_message=\"Training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648d9822-53c9-42cc-992c-d3589ae43d90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
